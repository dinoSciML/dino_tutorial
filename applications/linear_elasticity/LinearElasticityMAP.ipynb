{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Elasticity Deterministic Inverse Problem\n",
    "\n",
    "This notebook uses `hippylib` to solve a deterministic inverse problem for an uncertain elastic modulus in a linear elasticity problem\n",
    "\n",
    "\n",
    "## Linear elasticity\n",
    "\n",
    "We can derive linear elasticity as the minimization of strain energy\n",
    "\n",
    "$$\n",
    "\\min_u \\int_\\Omega \\psi(\\varepsilon(u)) dx - \\int_\\Omega f\\cdot u dx - \\int_{\\Gamma_N} t \\cdot n ds(x)\n",
    "$$\n",
    "where $\\varepsilon(u) = \\frac{1}{2}(\\nabla u + \\nabla u^T)$\n",
    "where $u$ is restricted to an appropriate space to satisfy given Dirichlet boundary conditions, $f$ is a body load, and $t$ is a prescribed traction condition. Additionally:\n",
    "\n",
    "$$\n",
    "\\psi(\\boldsymbol{\\varepsilon}) = \\frac{\\lambda}{2} \\bigl(\\mathrm{tr}(\\boldsymbol{\\varepsilon})\\bigr)^2 + \\mu \\,\\mathrm{tr}(\\boldsymbol{\\varepsilon}^2)\n",
    "$$\n",
    "\n",
    "The minimizer gives us the following system\n",
    "\n",
    "$$ -\\nabla \\cdot \\sigma(u) = f$$\n",
    "$$ u = u_D \\text{ on } \\Gamma_D$$\n",
    "$$ \\sigma \\cdot n = t \\text{ on } \\Gamma_N$$\n",
    "\n",
    "$$ \\sigma(u) = \\lambda (\\nabla \\cdot u) I + 2\\mu \\varepsilon(u)$$\n",
    "\n",
    "\n",
    "In our case we parametrize our model by the elastic modulus:\n",
    "\n",
    "$$ E = \\frac{\\mu(3\\lambda + 2\\mu)}{\\lambda + \\mu} $$\n",
    "\n",
    "and we assume the Poisson's ration $\\nu = \\frac{\\lambda}{2(\\lambda + \\mu)} = 0.4$ to be fixed.\n",
    "\n",
    "We generate training data samples of $E$ using Gaussian random fields $m$. We assume $E = 1 + \\exp(m)$, in order to maintain the coercivity (well-posedness) of the PDE.\n",
    "\n",
    "## Inverse Problem\n",
    "\n",
    "We have pointwise observations of the displacements $u \\in \\mathcal{U}$. The observation operator is $B:\\mathcal{U} \\rightarrow \\mathbb{R}^{d}$. The observational data $\\mathbf{d}$ are assumed to agree with the true observations up to some irreducible observational noise $\\xi \\sim \\mathcal{N}(0,\\Gamma)$. That is $\\mathbf{d} = Bu(m_\\text{true}) + \\xi$\n",
    "\n",
    "The minimization problem is \n",
    "\n",
    "$$ \\min_m \\frac{1}{2}\\|Bu(m) - \\mathbf{d}\\|^2_{\\Gamma^{-1}} + \\frac{1}{2}\\|m - m_0\\|^2_{\\mathcal{C}^{-1}}$$\n",
    "\n",
    "where $\\mathcal{C}^{-1}$ is a weighting for a Tikhonov regularization, and in the Bayesian context we identify it with the covariance of a Gaussian prior, e.g., $\\mu_\\text{prior} = \\mathcal{N}(m_0,\\mathcal{C})$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MIT License\n",
    "# Copyright (c) 2025\n",
    "#\n",
    "# This is part of the dino_tutorial package\n",
    "# \n",
    "# Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    "# of this software and associated documentation files (the \"Software\"), to deal\n",
    "# in the Software without restriction, subject to the following conditions:\n",
    "#\n",
    "# The above copyright notice and this permission notice shall be included in\n",
    "# all copies or substantial portions of the Software.\n",
    "#\n",
    "# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND.\n",
    "# For additional questions contact Thomas O'Leary-Roseberry\n",
    "\n",
    "import dolfin as dl\n",
    "import ufl\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys, os\n",
    "import time\n",
    "sys.path.append( os.environ.get('HIPPYLIB_PATH', \"../\") )\n",
    "import hippylib as hp\n",
    "\n",
    "from linear_elasticity_model import *\n",
    "\n",
    "# sys.path.append( os.environ.get('HIPPYFLOW_PATH'))\n",
    "# import hippyflow as hf\n",
    "\n",
    "import logging\n",
    "logging.getLogger('FFC').setLevel(logging.WARNING)\n",
    "logging.getLogger('UFL').setLevel(logging.WARNING)\n",
    "dl.set_log_active(False)\n",
    "\n",
    "np.random.seed(seed=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define some basic settings for \n",
    "settings = linear_elasticity_settings()\n",
    "# Define model\n",
    "model = linear_elasticity_model(settings)\n",
    "\n",
    "Vh = model.problem.Vh\n",
    "\n",
    "# Generate data:\n",
    "############\n",
    "noise = dl.Vector()\n",
    "model.prior.init_vector(noise,\"noise\")\n",
    "hp.parRandom.normal(1., noise)\n",
    "mtrue = dl.Vector()\n",
    "model.prior.init_vector(mtrue, 0)\n",
    "model.prior.sample(noise, mtrue)\n",
    "\n",
    "# mtrue = true_parameter(model.prior)\n",
    "# plot mtrue, mean \n",
    "objs = [dl.Function(model.problem.Vh[hp.PARAMETER],mtrue), \n",
    "        dl.Function(model.problem.Vh[hp.PARAMETER],model.prior.mean)]\n",
    "mytitles = [\"True Parameter\", \"Prior mean\"]\n",
    "hp.utils.nb.multi1_plot(objs, mytitles)\n",
    "plt.show()\n",
    "\n",
    "utrue = model.problem.generate_state()\n",
    "x = [utrue, mtrue, None]\n",
    "\n",
    "model.problem.solveFwd(x[hp.STATE], x)\n",
    "data = model.misfit.B*x[hp.STATE]\n",
    "# MAX = data.norm(\"linf\")\n",
    "# noise_std_dev = rel_noise * MAX\n",
    "hp.parRandom.normal_perturb(settings['noise_variance']**0.5, data)\n",
    "model.misfit.d = data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = dl.Vector()\n",
    "model.misfit.B.init_vector(q,0)\n",
    "print(q.get_local().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(15,11))\n",
    "u_norm = get_unorm(model, utrue)\n",
    "\n",
    "u_plot = dl.Function(Vh[hp.STATE])\n",
    "u_plot.vector().zero()\n",
    "u_plot.vector().axpy(1.,utrue)\n",
    "\n",
    "vmax = u_norm.vector().max()\n",
    "vmin = u_norm.vector().min()\n",
    "\n",
    "# Project onto a scalar function space. Often, u0.function_space() is a scalar subspace.\n",
    "\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.subplot(221)\n",
    "cbar = dl.plot(u_plot, mode=\"displacement\", title=\"Displacement\")\n",
    "_ = plt.colorbar(cbar)\n",
    "\n",
    "\n",
    "# hp.utils.nb.plot(u_norm, mytitle=\"Absolute Value (u)\", subplot_loc=221, vmin=vmin, vmax=vmax)\n",
    "# norm = np.linalg.norm(model.misfit.d.get_local().reshape((-1, 2)), axis=1)\n",
    "# tmp = dl.Vector(dl.MPI.comm_world, len(norm))\n",
    "# tmp.set_local(norm)\n",
    "# hp.utils.nb.plot_pts(model.targets, tmp, mytitle=\"Observations (u)\", subplot_loc=222, vmin=vmin, vmax=vmax)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl.plot(u_plot, mode=\"displacement\", title=\"Displacement\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hippylib.utils.nb as nb\n",
    "# Compute MAP point\n",
    "m = model.prior.mean.copy()\n",
    "solver = hp.ReducedSpaceNewtonCG(model)\n",
    "solver.parameters[\"rel_tolerance\"] = 1e-6\n",
    "solver.parameters[\"abs_tolerance\"] = 1e-12\n",
    "solver.parameters[\"max_iter\"]      = 100\n",
    "solver.parameters[\"GN_iter\"] = 20\n",
    "solver.parameters[\"globalization\"] = \"LS\"\n",
    "solver.parameters[\"LS\"][\"c_armijo\"] = 1e-4\n",
    "\n",
    "    \n",
    "x = solver.solve([None, m, None])\n",
    "    \n",
    "if solver.converged:\n",
    "    print( \"\\nConverged in \", solver.it, \" iterations.\")\n",
    "else:\n",
    "    print( \"\\nNot Converged\")\n",
    "\n",
    "print( \"Termination reason: \", solver.termination_reasons[solver.reason] )\n",
    "print( \"Final gradient norm: \", solver.final_grad_norm )\n",
    "print( \"Final cost: \", solver.final_cost )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot MAP point\n",
    "plt.figure(figsize=(15,5))\n",
    "nb.plot(dl.Function(model.problem.Vh[hp.PARAMETER],mtrue), subplot_loc=121,mytitle=\"True Parameter\")\n",
    "nb.plot(dl.Function(model.problem.Vh[hp.PARAMETER], x[hp.PARAMETER]), subplot_loc=122,mytitle=\"MAP Parameter\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.setPointForHessianEvaluations(x, gauss_newton_approx=False)\n",
    "Hmisfit = hp.ReducedHessian(model, misfit_only=True)\n",
    "k = 50\n",
    "p = 20\n",
    "print( \"Single/Double Pass Algorithm. Requested eigenvectors: {0}; Oversampling {1}.\".format(k,p) )\n",
    "\n",
    "Omega = hp.MultiVector(x[hp.PARAMETER], k+p)\n",
    "hp.parRandom.normal(1., Omega)\n",
    "lmbda, V = hp.doublePassG(Hmisfit, model.prior.R, model.prior.Rsolver, Omega, k)\n",
    "\n",
    "posterior = hp.GaussianLRPosterior(model.prior, lmbda, V)\n",
    "posterior.mean = x[hp.PARAMETER]\n",
    "\n",
    "plt.plot(range(0,k), lmbda, 'b*', range(0,k+1), np.ones(k+1), '-r')\n",
    "plt.yscale('log')\n",
    "plt.xlabel('number')\n",
    "plt.ylabel('eigenvalue')\n",
    "\n",
    "nb.plot_eigenvectors(model.problem.Vh[hp.PARAMETER], V, mytitle=\"Eigenvector\", which=[0,1,2,5,10,15])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
